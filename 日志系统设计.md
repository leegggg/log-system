#日志系统设计
##日志系统云平台层设计
MSCP微服务云平台的整体架构如下图所示
![alt text](mscp_arch.png "Arch mscp")
在MSCP平台上，每个zone对应着一个kubernetes集群，这个集群一般被安置在同一个服务器机房的内部；而同一个region中的各个不同zone一般部署在同一个数据中心内部，其间一般由高速网络相连接（一般有多个千兆网络）。而region一般对应用户部署在不同地理位置上的数据中心，如用户的北京数据中心，纽约数据中心等等。其间一般通过公用网络相连接，这些链接一般容量不足，同时价格昂贵，无法支持大量日志数据的传输需求。针对以上情况我们对日志系统在云平台上的部署作了如下图的规划：
![alt text](log_topology.png "Topology log")
每个region拥有自己的ElasticSearch数据库集群，该集群与MSCP平台的其他基础组件一同被安装在一个被称为Admin zone的kubernetes集群之中。用户无法在此zone中部署自己的应用。这样的设计保证了MSCP平台的稳定性和用户体验。同一个region中的日志收集器和分析器均将数据发送到region中的ElasticSearch中。而日志查询API Server能够根据用户查询的不同从不同的数据源中查询日志数据。
##日志系统总体架构设计
日志系统架构如下图所示：
* 数据收集组件：日志收集器，HTTP Restful API接口
* 数据清洗组件：logstash分析器
* 数据传递组件：高速消息队列缓存kafka
* 数据存储组件：ElasticSearch
* 数据查询组件：数据查询API Server
* 前端UI
![alt text](log_arch.png "Arch log")
数据收集组件从docker容器内部收集包括stdout/stderr，日志文件等在内的日志数据，并且开放HTTP API接口以供用户应用直接推送日志数据。该API接口能够通过分析数据来源自动将这些数据归入某用户名下从而实现数据隔离。logstash分析器对来自各个数据源的数据进行清洗，删除一些不需要的字段，同时对数据来源作分析，形成半结构化的数据，传递给高速缓存kafka。由于logstash作为无状态的服务，可以几乎无限地水平扩展，其性能往往远远超过ElasticSearch落盘的性能。Kafka高速缓存可以有效地将到达的数据缓存起来，等待ElasticSearch空闲时再将数据落盘。这样的设计一方面挡住了高并发的访问减少了日志数据落盘时的阻塞，另一方面有效地利用了ElasticSearch的性能。作为MSCP平台API接口的一部分，日志服务也提供了restful的API接口，该接口兼容ElasticSearch查询语法，同时背后的API Server能够有效地对用户数据进行隔离。在MSCP前端页面上我们也提供了一个前端页面，用户可以方便地通过这个页面查询自己应用的日志。下面详细说明各个组件的设计方案。
##日志系统各组件设计
###数据清洗组件
该组件主要负责对日志的数据进行清洗，为了能够在